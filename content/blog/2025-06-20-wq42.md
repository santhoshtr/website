---
title: "wq42: LLM meets Wikidata"
author: Santhosh Thottingal
type: post
draft: true
date: 2025-06-20T05:00:00+05:30
url: /blog/2025/06/20/wq42-llm-wikidata/
categories:
  - Wikipedia
  - Wikidata
  - LLM
---

Integrating Knowledge graphs(KG) to Large language models(LLM) is a well explored field in research. Knowledge Graphs are large structured databases that store factual associations as edges in a graph. Such integrations makes LLMs capable of question answering and such tasks based on structured and often up to date information available in KGs and thereby reducing the chances of hallucination. For example, if an LLM can rely in Wikidata, a well known KG project, instead of relying on the facts learned from its training data, it becomes more reliable and useful.

In the paper titled "[Large Language Models, Knowledge Graphs and Search Engines: A Crossroads for Answering Users’ Questions](https://arxiv.org/html/2501.06699v1)", co-authored by Denny Vrandečić , founder of Wikidata examines
this problem and introduces a taxonomy of user information needs, which guides us to study the pros, cons and possible synergies of Large Language Models, Knowledge Graphs and Search Engines. This paper argues that these technologies are complimentary to each other and can benefit by integrating them together to overcome the weakness of one with the strength of other.

![KG-SE-LLM](/wp-content/uploads/2025/06/Screenshot_20-Jun_14-19-54_113.png)
(Image credits: [Large Language Models, Knowledge Graphs and Search Engines: A Crossroads for Answering Users’ Questions](https://arxiv.org/html/2501.06699v1))

There is plenty of published research on various approaches for integrating KGs with LLMs. I cannot list all of them here, but to give an idea, let me list a few approaches.

1. Textualize LLMs into a format like json, text, markdown or even custom format. Emebed this using Vector embedding methods. For a task like question answering, do Retrieval Augmented Generation(RAG). In general, this involves placing the retrieved KG (textualized) in prompt of an LLM in later stage. So they are also called prompt based methods. LLM by itself is not trained or finetuned with KG

![KG-textualized-RAG](/wp-content/uploads/2025/06/textualized-kg-rag.png)

(Image credits: <https://knowledge-nlp.github.io/naacl2025/papers/39.pdf>)

2. Parameterized or model integrated KGs - In this approach, LLM model is fined tuned with KGs. For example, [KG-Adapter](https://aclanthology.org/2024.findings-acl.229.pdf), a parameter-level KG integration method based on parameter- efficient fine-tuning (PEFT). Another example is described in [Injecting Knowledge Graphs into Large Language Models](https://arxiv.org/html/2505.07554v1) which integrate graph embeddings within the LLM input as tokens, we extend this paradigm to the KG domain by leveraging Knowledge Graph Embedding (KGE) models.

![KG-Adapter](/wp-content/uploads/2025/06/kg-adapter.png)

(Image credits: <https://aclanthology.org/2024.findings-acl.229.pdf>)

In either of these approaches, preparing embedding models for RAG or fine-tuning LLMs, it involves huge effort, cost and additional maintenance costs.

In this article, I am explaining a similar but alternate approach that avoids embedding KGs and capable of basic level analytical ability using tool calling LLMs(Also known as agentic AI). I will present a tool that allows user to ask natural language questions against wikidata.

> For readers not interested in learning about the technology behind, go ahead and try yourself at <https://wq42.toolforge.org>

## Textualization

Textualizing, the process of converting the knowledge graph into a textual representation so that ML techiques like vector embedding or using it in a prompt for LLM is the hardest part in the case of Wikidata. There is no straight forward API to get all the information associated with a particular Qid(example: Q405 - Moon). In the last two articles I explained how I prepared a web API to get json formatted information about a given item and how I used that to prepare an LLM usable markdown format.

- [qjson: Fetching all properties of a wikidata item in a single API call](https://thottingal.in/blog/2025/04/15/qjson/)
- [qrender: Render wikidata item in different formats](https://thottingal.in/blog/2025/06/16/qrender/)

This paper - **KG-LLM-Bench: A Scalable Benchmark for Evaluating LLM Reasoning on Textualized Knowledge Graphs** <https://knowledge-nlp.github.io/naacl2025/papers/39.pdf?s=35>, evaluates various data formats suitable for textualization. I read this after I write qrender. I used markdown format, with links, images, headings to represent the information associated with given QId. A unique aspect of my approach is I also do logical grouping of properties so that similar properties are grouped together. For example, in the case of person, names(first, last, family, official, nick), family details, jobs, awards etc are grouped together. I found this very important and affects the output quality of LLMs. The scattered information or presence of distracting information has a big impact on LLMs generation - Recently this was described as '[context rot](https://news.ycombinator.com/item?id=44308711&s=35#44310054)'

## Tool calling

## Code execution

## Examples and screenshots

## Source code

## Disclaimer

I work at the Wikimedia Foundation. However, this project, its exploration, and the opinions expressed are entirely my own and do not reflect my employer's views. This is not an official Wikimedia Foundation project.
