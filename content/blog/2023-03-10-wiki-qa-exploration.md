---
title: "Natural language question answering in Wikipedia - an exploration"
author: Santhosh Thottingal
type: post
date: 2023-03-10T9:45:00+05:30
url: /blog/2023/03/10/wikiqa
categories:
  - wikipedia
tags:
  - wikipedia

---


In this blog post I explain the prospects of providing questions and answers as an additional content format in wikipedia, a [human-in-the-loop](https://hai.stanford.edu/news/humans-loop-design-interactive-ai-systems) approach for that with a prototype.

## Introduction

Wikipedia is the destination of curiosity. Curiosity starts with a question. People visit wikipedia when searching answer for that question. However, people come to wikipedia after intermediaries like search engines, that shows the source of answer to them. Wikipedia by itself is not much equipped to answer the questions other than the limited keyword based search.

With the recent advancements in AI based conversational agents such as ChatGPT, finding information using natural language questions are increasingly becoming a popular method for knowledge seekers. However, the answers given by AI based applications can be false or made up information due to the nature of those systems.

So, there is a need for reliable, verifiable information that can be queried using natural language questions. The editor community of Wikipedia created the largest encyclopedia. If this knowledge based can be utilized to serve natural language queries, it would be quite useful for many people.

In the current state of natural language processing, the straight forward solution that will come to our mind would be to provide an AI based solution that answer all questions based on Wikipedia content. Baring all technical challenges, let us just assume, we build such a solution. For me, this approach does not go very well with the wikipedia's principles and practices. Why?

First of all, it is quite possible that the probabilistic answers such a solution gives contain false information even when correct information is present in Wikipedia content. At Wikimedia Foundation, one of my colleuage tested this by providing Wikipedia article to ChatGPT and then asking questions to it.

The content in wikipedia is always editable. But the answers resulting from these AI solutions are not editable. They are output from a blackbox. One cannot explain it. Humans are not involved in answering, the answers are not human approved.

Serving a few languages that are supported by current Large language models are not enough for Wikipedia. Wikipedia supports 300+ languages.

How can we use the language capabilities of current large language models and at the same time have a question answering system that adheres to wikipedia's principles?

## Proposed solution

Let us have a new content format associated with every article: "Questions". This would be very similar to current Talk namespace where people have discussions about article. The Questions namespace will have all possible questions related to article content and answers. People canedit the questions, answers. People can ask questions and others can answer it. The answers can have everything a wikipedia article can have - references, images etc. Every Question answer unit can have its own revision, and edit workflows like reverts and vandalism checks.

![Listing of all questions in Hydrogen article](/wp-content/uploads/2023/03/hydrogen-questions.jpg)

You might be thinking that this is asking for a manual effort by editors to have all these possible questions and answers created, while they already spent their valuable time in editing the article content and content here are repetitions of same in different format.

But what if, if we can generate all these questions using a large language model and then humans just verify them and publish? Wikipedia has [article translation tool](https://www.mediawiki.org/wiki/Content_translation) that already use this human-in-loop method - translations for section are provided by machine translation engines and editors verify or edit it before it actually get published. Translator can also discard, or not use the translation proposal by machine translation engines and write their own translation from scratch. We can apply the same principle here.

However, I already mentioned that LLM's answers are not completely reliable when article content is provided and then questions are asked on it. I found a solution for this problem by inverting the steps. Instead of asking a question based on the content, I asked the LLM to generate all possible questions from the content and give answers to it.  This makes sure that answers are always extracts from the article content(It is rephrased as answer to a question) and questions are generated by LLM. From my experiments, I could not find any issues of hallucination. Here is the prompt I used with gpt-3.5-turbo model

```
Act as if no information exists in the universe other than what is in this text:

(wiki article text goes here)

Provide all possible questions and answers in json format with "question" and "answer" as keys.
```

*I asked for json format just to avoid parsing plain text output from the model. It did provide json, but sometimes as array of questions answers, sometimes with a top level key 'questions' and value as array of questions and answers. Found it unpredictable, but it was not difficult to detect the format of json and act accordingly*

## Protoype

To illustrate the concept, I build a prototype. To capture the generated questions and answers and expose it as API, I wrote a server application. Source code: https://github.com/santhoshtr/wq and it has a simple web interface that runs at https://wq.thottingal.in. Note that this use the gpt-3.5-turbo API by OpenAI and I don't promise it will run all the time. I will at least keep it running till I use up free API credit I received.

![wq screenshot](/wp-content/uploads/2023/03/wq-screenshot.jpg)

Then I integrated the API to Wikipedia articles using [a simple userscript](https://en.wikipedia.org/wiki/User:Santhosh.thottingal/wq.js)

And here is a video that showing "Questions" namespace listing all questions, allowing edit and adding questions.

<iframe width="100%" height="400" src="https://www.youtube.com/embed/4KxjfHwUs-4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

## Prospects

* The [human-in-the-loop](https://hai.stanford.edu/news/humans-loop-design-interactive-ai-systems) approach of having questions and answers is as per Wikipedia's principles. This approach does not expose the results of an AI tool to readers without human approval. At the same time, the usage of LLM here saves manual effort and relies editors only for validations.
* Allowing people to ask questions opens up new ways of engagement and contributions.
* We are not limited by the languages supported by LLM as smaller languages can build the questions and answers list with the help of translation tool(that also require human validation). The proposed approach provides adding question and answer manually without relying on any tool.
* Enhancements like voting or methods to find most useful questions can become "Frequently asked questions" for the article
* Other content formats can be phrased as question and can be served by same system. For example, a summary of a large Wikipedia article is "Please provide a summary of this article" question. Also, we can have a question: "Explain this like I am 5 year old"
* Exposing the questions and answers through internal search engine is first step towards natural language querying. External search engines will also index them.

## Disclaimers

* This is just a prototype of a concept and does not discard all product design and community interaction efforts that need to happen, in case this becomes reality one day. I am curious to hear various prospects, product design ideas you may have.
* I work at Wikimedia Foundation. However, this proposal and approach are my personal views and does not reflect my employers view. I had written and shared most of this concepts in 2019 in a document to my colleagues and it had this note: "Later we amend the system with machine generated questions and answers as well." :-)

Have fun!
